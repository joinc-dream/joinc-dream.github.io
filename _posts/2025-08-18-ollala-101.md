---
layout: single 
title: "Local LLM 호스팅을 위한 ollma 가이드"
date: 2025-08-19 01:00:00 +0900
categories: 
card_description: "Local LLM 호스팅을 위한 ollama 가이드"
author: sangbaeyun 
---
## Ollama ?
Ollama는 LLM(거대 언어 모델)을 쉽고 편리하게 실행하고 관리 할 수 있도록 도와주는 **추론엔진을 포함한 종합적인 플랫폼**입니다. 

LLM의 전체 생명주기는 학습(Training) 과 추론(Interence)로 나누어 볼 수 있습니다. 메타나 구글, OpenAI 등에서 학습한 LLM을 배포하면, **추론** 엔진을 이용해서 사용하는 방식입니다. 즉 추론이란 이미 만들어서 배포한 모델을 활용해 사용자의 질문에 답하거나 글을 요약하는 등 **실질적인 작업을 수행** 하는 것이라고 보면 됩니다. Ollama 같은 툴을 사용하면 LLM을 쉽게 사용 하는 환경을 만들 수 있습니다. 

## 왜 Ollama 를 사용해야 하나요 ?
아래와 같은 이유로 Ollama를 사용해야 한다고 주장하곤 합니다.
  * 괜찮은 답변을 얻기 위해서 ChatGPT를 비롯하여 다양한 모델에 지불해야 하는 비용들
  * 개인 데이터를 클라우드에 보내는 건 위험하지 않나요
  * AI 서비스를 사용하기 위해서 인터넷에 연결되어 있어야 하는 불편한 상황 

하지만 솔직히 위의 이유들은 **ollama를 사용해야 하는 이유를 만들기 위한 핑계** 라고 생각합니다. 1년 이상 꾸준히 ollama를 사용하고 있지만 위의 이유로 ollama를 사용하고 있지는 않습니다. 비용 때문에 Gemini나 ChatGPT를 사용 못할 정도는 아닐 뿐더러, ollama로 실행해야 하는 작은 크기의 모델들로 원하는 응답 성능을 뽑아낼 수 없기 때문입니다.

Google Gemini, OpenAI GPT 의 경우 GDPR, CCPA등을 준수하고 있으며, 모델에 입력된 개인 대화 내용을 모델 학습(재훈련)에 사용하지 않는다고 하고 있으니까요. 그걸 믿을 수 있느냐 라고 물어볼 수도 있겠지만 이걸 믿지 못하면 인터넷 서비스를 사용하지 말아야겠죠(회사 데이터를 가지고 사용하는 경우에는 다르긴 합니다만). 

저는 ollama는 순전히 개발자 관점에서 저에게 도움을 주기 때문에 사용 할 뿐입니다.

  * 로컬 실행: GPU/CPU 로컬 환경에서 실행 할 수 있습니다. 여러 전문적인 AI Agent를 오케스트레이션 해야 하는 시스템을 고민히야 하기 때문입니다. 
  * 경량 모델 지원: 20B 이하의 작은 모델들을 빠르게 테스트 할 수 있습니다.
  * 빠른 실행: `ollama run` 명령으로 즉시 실행 가능 합니다.
  * 개발 친화성: REST API, CLI, OpenWebUI 등을 지원하며 빠르게 앱/서비스에 통합 할 수 있습니다.
  * 멀티모델 관리: 여러 LLM을 로컬에서 스위칭하면서 테스트 할 수 있습니다.
  * 커뮤니티 생태계: 주요 모델들은 한달, 빠르면 몇 주이내에 사용 할 수 있습니다.  

제가 생각하는 Agentic AI의 모습은 아래와 같습니다. 각 업무 별로 특화된 Agent가 있으며 이를 오케스트레이션 하는 Agent가(이 Agent는 의도를 분석하는 일을 할겁니다) 통합하는 이런 구성이죠. 이런 구성에서는 도메인 특화된 온프레미스 환경에서 실행되는 Agent 들도 함께 구성이 될겁니다. AI 엔지니어인 저는 지금 부터 ollama를 이용해서 테스트 해 볼 필요가 있는 거죠.

![Agentic AI](https://docs.google.com/drawings/d/e/2PACX-1vQ1Mz80dEGC27v06V-DaGEwNlRaN0FIufBjbuWyf7T7ad2lV4q2ZmXXmaDjovRMVlXrLhXd8wNFk2b5/pub?w=960&h=720)


## 오늘 할 일
 * Ollama 를 설치합니다. 
 * Google의 Gemma3 4B 모델을 설치합니다. 
 * OpenWebUI를 이용해서 Gemma3 4B 모델을 테스트합니다.

## Ollama 설치
#### Linux 및 macOS: CLI 로 설치
터미널을 좋아한다면 **curl**을 이용해서 설치 할 수 있습니다.
```
curl -fsSL https://ollama.com/install.sh | sh
```
설치를 끝낸 후 간단히 테스트를 해보겠습니다.
```
ollama --version
ollama version is 0.11.4
```
ollama 의 기본 서비스 포트는 11434 입니다. curl로 테스트해볼 수 잇습니다.
```
curl localhost:11434
Ollama is running%
```

#### 첫번째 모델 다운로드 하기
엔진을 설치했으니, 이제 작업에 적합한 LLM를 선택해야 합니다. 수천/수만개의 LLM들이 있는데, 다행히 Ollama는 다양한 목적과 크기를 가진 **공식 라이브러리**를 제공하고 있습니다. 여기에서 원하는 모델을 다운로드 해서 사용하면 됩니다. 

🔗 [Ollama 공식 Library](https://ollama.com/library) 페이지에서 LLM을 검색 할 수 있습니다. 

우선은 작은 모델을 먼저 테스트해보겠습니다. 이번에는 작지만 강력한 그리고 무엇보다 한글도 잘 인식하는 구글의 Gemma3 4B 모델을 설치해보겠습니다. 

![Gemma3](https://docs.google.com/drawings/d/e/2PACX-1vShLtQ-zL_cEDUwfvHn-tuDOEol7M4tlaW_QUP2MPOUFdgcMml90bNGX_C7TYF71XcGDp4l6S0N7GB7/pub?w=960&h=720)

Gemma3 4B 모델은 약 40억개의 파라미터를 가집니다. Ollama는 보통 4비트 양자화(q4)된 모델을 사용하는데, 이 경우 모델 자체를 불러오는데 대략 4GB 크기의 VRAM 을 가진 GPU가 필요합니다. 참고로 제 PC의 사양은 아래와 같습니다.
  * CPU: AMD Ryzen 5 5600X 6-Core
  * Memory: 32G
  * GPU: NVIDIA GeForce RTX 4060 Ti 16G 
가장 중요한 것은 GPU 가 되겟습니다. 16G 정도면 4비트 양자화된 20B 크기의 모델도 실행 할 수 있기 때문에 적당한 가성비 모델로 사용 할 수 있습니다.  

ollama 주요 명령들을 정리했습니다.
```
# 1단계: 모델 다운로드
# gemma3:4b 모델을 다운로드 합니다. 시스템 사양이 좋지 않다면 gemma3:1b를 다운로드 해도 됩니다.
ollama pull gemma3:4b

# 2단계: 모델 실행
# 대화형 채팅이 열립니다.

# 3단계: 다운로드 한 모델목록을 나열합니다. 
ollama list

# 4단계: 실행 중인 모델을 확인합니다.
ollama ps

# 5단계: 모델을 중지 합니다.
ollama stop gemma3:4b

# 모델 삭제
ollama rm gemma3:4b
```

![Gemma3 다운로드](https://docs.google.com/drawings/d/e/2PACX-1vQkevYSdcq_pnRkOSppsifPpgJzH7y_19r1m_29zP5OLsZ6lTkxJz-dkGqY5uibutvYLOpW68FwZvFS/pub?w=960&h=720)

## Ollama API 사용하기
Ollama는 API를 제공하며 이를 이용해서, 다양한 앱 개발이 가능합니다. 
 * 내장 API 서버: Ollama는 별도의 설정 없이 http://localhost:11434 로 기본적인 백앤드 API 서버를 실행합니다. 
 * API를 이용해서 즉시 모델을 교체해서 사용 할 수 있습니다.

예를 들어서 `GET /api/tags` API를 이용하면 모델 목록을 가져올 수 있습니다.
```
curl http://localhost:11434/api/tags | json_pp
{
   "models" : [
      {
         "details" : {
            "families" : [
               "gemma3"
            ],
            "family" : "gemma3",
            "format" : "gguf",
            "parameter_size" : "4.3B",
            "parent_model" : "",
            "quantization_level" : "Q4_K_M"
         },
         "digest" : "a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a",
         "model" : "gemma3:4b",
         "modified_at" : "2025-08-17T22:57:08.540920901+09:00",
         "name" : "gemma3:4b",
         "size" : 3338801804
      },
      {
         "details" : {
            "families" : [
               "gptoss"
            ],
            "family" : "gptoss",
            "format" : "gguf",
            "parameter_size" : "20.9B",
            "parent_model" : "",
            "quantization_level" : "MXFP4"
         },
... 생략

```

기본적으로 Ollama의 API는 상태 비저장입니다. 즉, 모든 요청은 완전히 새로운 대화입니다. 사람처럼 대화하는 **멀티턴** 대화가 필요하다면, 코드로 이를 구현해야 합니다. 간단한 Python 코드 만들어보겠습니다.

## Ollama API를 이용한 LLM 애플리케이션
```Python
import requests

# Ollama's default API endpoint
OLLAMA_URL = "http://localhost:11434/api/generate"

def chat_with_ai(prompt, model="gemma3:4b"):
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False  # Disable streaming for simplicity
    }
    response = requests.post(OLLAMA_URL, json=payload)
    return response.json()["response"]

# Example usage
ai_response = chat_with_ai("서울에서 가장 유명한 관광지 3군데 알려줘.")
```
🤖 AI says: 서울에서 가장 유명한 관광지 3곳은 다음과 같습니다.
  1.  **경복궁 (Gyeongbokgung Palace):** 조선 시대의 대표적인 궁궐로, 아름다운 건축물과 넓은 정원을 자랑합니다. 한복을 입고 궁궐을 거닐며 한국의 역사와 문화를 느껴볼 수 있습니다. 특히, 계절마다 다른 아름다움을 느낄 수 있는 꽃청춘 행사나 다양한 축제가 열리는 곳입니다.
  2.  **명동 (Myeongdong):** 쇼핑과 먹거리가 가득한 활기 넘치는 거리입니다. 다양한 브랜드 매장, 길거리 음식, 화장품 가게 등 볼거리와 즐길 거리가 다양합니다. 특히, 밤에는 화려한 조명으로 쇼핑몰이 더욱 아름답게 빛납니다.
  3.  **남산 (Namsan Mountain):** 서울 시내를 한눈에 내려다볼 수 있는 곳입니다. 케이블카를 타고 올라갈 수도 있으며, 남산타워에는 전망대와 레스토랑이 있습니다.  산책로를 따라 다양한 식물과 조형물을 감상하며 휴식을 취할 수 있습니다.
이 외에도 이화마을, 인사동, 종묘 등 서울에는 다양한 명소들이 많으니, 취향에 맞는 곳을 선택하여 즐거운 여행을 즐기세요!

**Ollama Package**를 이용해서 코드를 만들 수도 있습니다.
```Python
# -*- coding: utf-8 -*-
from ollama import chat

# First call - No memory
response = chat(model='gemma3:4b', messages=[
    {'role': 'user', 'content': '왜 하늘은 파란가요?'}
])
print(response.message.content)
```

물론 **LangChain** 도 지원한다.
```Python
# -*- coding: utf-8 -*-
from langchain_community.llms import Ollama

# Initialize Ollama with your chosen model
llm = Ollama(model="gemma3:4b")

prompt_1 = '상대성 이론에 대해서 설명해주세요'

response = llm.invoke(prompt_1)
print(response)
```

## OpenWebUI
OpenWebUI는 LLM 사용과 테스트를 도와주는 웹 애플리케이션입니다. OpenWebUI를 이용하면 OpenAI, Gemini 뿐만 아니라 Ollama 까지 손쉽게 사용 할 수 있습니다. 

```
docker run -d --network="host" -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```
![OpenWebUI](https://docs.google.com/drawings/d/e/2PACX-1vQkzS0hsBwzL2jRraQ16Q_M4eFBGg1HZyXrv0FFjB3c3_BE_hK5pxPKFcqnxyNt9v9v-79e33MB_i6S/pub?w=960&h=720)

"사용자 아이콘 클릭 > 관리자 설정 > 연결" 에서 Ollama API Endpoint를 설정 할 수 있다. ⚙️ 버튼을 클릭해서 Ollama API 주소를 "http://localhost:11434"로 바꾸고 "연결 테스트"까지 진행해보자.
![OpenWebUI connection](https://docs.google.com/drawings/d/e/2PACX-1vSSSOemDBsPe6ircPcNnWbyHsG8oxwpNnlw7w8eC6NjiJKPZ5FmrQf9d5kOgTXM3Vh6jYNrb6YWVs99/pub?w=960&h=720)

아래와 같이 ollama에 설치된 모델들을 선택해서 AI와 대화 할 수 있다.
![OpenWebUI Ollama 모델 선택](https://docs.google.com/drawings/d/e/2PACX-1vRNqLt-v1DVwyZ_HL1qtbzDgKPPfnlhN5BdprEb2bVAon2QxmW_l5Yl-vOfp4k8JPKb6_sdMLla-zeL/pub?w=960&h=720)

## 다음에 할 일들 
 * llama3, Qwen 과 같은 다른 모델들을 테스트해보세요
 * LangChain/LanGraph의 retrieval, agents, memory 기능을 이용해서 LLM 애플리케이션을 만들어보세요.
 * 팀을 위한 로컬 기반의 챗봇을 배포해보세요.
