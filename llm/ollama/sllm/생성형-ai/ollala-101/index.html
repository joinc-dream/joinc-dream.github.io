<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.3 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">

<head>
  <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Local LLM 호스팅을 위한 ollma 가이드 - JOINC AI TeamBlog</title>
<meta name="description" content="Ollama ? Ollama는 LLM(거대 언어 모델)을 쉽고 편리하게 실행하고 관리 할 수 있도록 도와주는 추론엔진을 포함한 종합적인 플랫폼입니다.">


  <meta name="author" content="Sang Bae YUN">
  
  <meta property="article:author" content="Sang Bae YUN">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="JOINC AI TeamBlog">
<meta property="og:title" content="Local LLM 호스팅을 위한 ollma 가이드">
<meta property="og:url" content="/llm/ollama/sllm/%EC%83%9D%EC%84%B1%ED%98%95-ai/ollala-101/">


  <meta property="og:description" content="Ollama ? Ollama는 LLM(거대 언어 모델)을 쉽고 편리하게 실행하고 관리 할 수 있도록 도와주는 추론엔진을 포함한 종합적인 플랫폼입니다.">







  <meta property="article:published_time" content="2025-08-19T01:00:00+09:00">





  

  


<link rel="canonical" href="/llm/ollama/sllm/%EC%83%9D%EC%84%B1%ED%98%95-ai/ollala-101/">












<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="JOINC AI TeamBlog Feed">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



  <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  <link rel="stylesheet" href="/assets/css/style.css">
</head>

<body
  class="layout--single"
  dir="ltr">

  <div class="initial-content">
    
  
<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          JOINC AI TeamBlog
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/about/"
                
                
              >About</a>
            </li><li class="masthead__menu-item">
              <a
                href="/blog/"
                
                
              >Blog</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>







<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="/">
        <img src="/assets/images/bio-photo-yun.webp" alt="Sang Bae YUN" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="/" itemprop="url">Sang Bae YUN</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>세상을 바꾸는 개발자, 윤상배 입니다.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">대한민국 서울</span>
        </li>
      

      
        
          
            <li><a href="mailto:yundream@gmail.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
            <li><a href="https://github.com/yundream" rel="nofollow noopener noreferrer me" itemprop="sameAs"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Local LLM 호스팅을 위한 ollma 가이드">
    <meta itemprop="description" content="Ollama ?Ollama는 LLM(거대 언어 모델)을 쉽고 편리하게 실행하고 관리 할 수 있도록 도와주는 추론엔진을 포함한 종합적인 플랫폼입니다.">
    <meta itemprop="datePublished" content="2025-08-19T01:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="/llm/ollama/sllm/%EC%83%9D%EC%84%B1%ED%98%95-ai/ollala-101/" itemprop="url">Local LLM 호스팅을 위한 ollma 가이드
</a>
          </h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          8 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <h2 id="ollama-">Ollama ?</h2>
<p>Ollama는 LLM(거대 언어 모델)을 쉽고 편리하게 실행하고 관리 할 수 있도록 도와주는 <strong>추론엔진을 포함한 종합적인 플랫폼</strong>입니다.</p>

<p>LLM의 전체 생명주기는 학습(Training) 과 추론(Interence)로 나누어 볼 수 있습니다. 메타나 구글, OpenAI 등에서 학습한 LLM을 배포하면, <strong>추론</strong> 엔진을 이용해서 사용하는 방식입니다. 즉 추론이란 이미 만들어서 배포한 모델을 활용해 사용자의 질문에 답하거나 글을 요약하는 등 <strong>실질적인 작업을 수행</strong> 하는 것이라고 보면 됩니다. Ollama 같은 툴을 사용하면 LLM을 쉽게 사용 하는 환경을 만들 수 있습니다.</p>

<h2 id="왜-ollama-를-사용해야-하나요-">왜 Ollama 를 사용해야 하나요 ?</h2>
<p>아래와 같은 이유로 Ollama를 사용해야 한다고 주장하곤 합니다.</p>
<ul>
  <li>괜찮은 답변을 얻기 위해서 ChatGPT를 비롯하여 다양한 모델에 지불해야 하는 비용들</li>
  <li>개인 데이터를 클라우드에 보내는 건 위험하지 않나요</li>
  <li>AI 서비스를 사용하기 위해서 인터넷에 연결되어 있어야 하는 불편한 상황</li>
</ul>

<p>하지만 솔직히 위의 이유들은 <strong>ollama를 사용해야 하는 이유를 만들기 위한 핑계</strong> 라고 생각합니다. 1년 이상 꾸준히 ollama를 사용하고 있지만 위의 이유로 ollama를 사용하고 있지는 않습니다. 비용 때문에 Gemini나 ChatGPT를 사용 못할 정도는 아닐 뿐더러, ollama로 실행해야 하는 작은 크기의 모델들로 원하는 응답 성능을 뽑아낼 수 없기 때문입니다.</p>

<p>Google Gemini, OpenAI GPT 의 경우 GDPR, CCPA등을 준수하고 있으며, 모델에 입력된 개인 대화 내용을 모델 학습(재훈련)에 사용하지 않는다고 하고 있으니까요. 그걸 믿을 수 있느냐 라고 물어볼 수도 있겠지만 이걸 믿지 못하면 인터넷 서비스를 사용하지 말아야겠죠(회사 데이터를 가지고 사용하는 경우에는 다르긴 합니다만).</p>

<p>저는 ollama는 순전히 개발자 관점에서 저에게 도움을 주기 때문에 사용 할 뿐입니다.</p>

<ul>
  <li>로컬 실행: GPU/CPU 로컬 환경에서 실행 할 수 있습니다. 여러 전문적인 AI Agent를 오케스트레이션 해야 하는 시스템을 고민히야 하기 때문입니다.</li>
  <li>경량 모델 지원: 20B 이하의 작은 모델들을 빠르게 테스트 할 수 있습니다.</li>
  <li>빠른 실행: <code class="language-plaintext highlighter-rouge">ollama run</code> 명령으로 즉시 실행 가능 합니다.</li>
  <li>개발 친화성: REST API, CLI, OpenWebUI 등을 지원하며 빠르게 앱/서비스에 통합 할 수 있습니다.</li>
  <li>멀티모델 관리: 여러 LLM을 로컬에서 스위칭하면서 테스트 할 수 있습니다.</li>
  <li>커뮤니티 생태계: 주요 모델들은 한달, 빠르면 몇 주이내에 사용 할 수 있습니다.</li>
</ul>

<p>제가 생각하는 Agentic AI의 모습은 아래와 같습니다. 각 업무 별로 특화된 Agent가 있으며 이를 오케스트레이션 하는 Agent가(이 Agent는 의도를 분석하는 일을 할겁니다) 통합하는 이런 구성이죠. 이런 구성에서는 도메인 특화된 온프레미스 환경에서 실행되는 Agent 들도 함께 구성이 될겁니다. AI 엔지니어인 저는 지금 부터 ollama를 이용해서 테스트 해 볼 필요가 있는 거죠.</p>

<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vQ1Mz80dEGC27v06V-DaGEwNlRaN0FIufBjbuWyf7T7ad2lV4q2ZmXXmaDjovRMVlXrLhXd8wNFk2b5/pub?w=960&amp;h=720" alt="Agentic AI" /></p>

<h2 id="오늘-할-일">오늘 할 일</h2>
<ul>
  <li>Ollama 를 설치합니다.</li>
  <li>Google의 Gemma3 4B 모델을 설치합니다.</li>
  <li>OpenWebUI를 이용해서 Gemma3 4B 모델을 테스트합니다.</li>
</ul>

<h2 id="ollama-설치">Ollama 설치</h2>
<h4 id="linux-및-macos-cli-로-설치">Linux 및 macOS: CLI 로 설치</h4>
<p>터미널을 좋아한다면 <strong>curl</strong>을 이용해서 설치 할 수 있습니다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl -fsSL https://ollama.com/install.sh | sh
</code></pre></div></div>
<p>설치를 끝낸 후 간단히 테스트를 해보겠습니다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ollama --version
ollama version is 0.11.4
</code></pre></div></div>
<p>ollama 의 기본 서비스 포트는 11434 입니다. curl로 테스트해볼 수 잇습니다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl localhost:11434
Ollama is running%
</code></pre></div></div>

<h4 id="첫번째-모델-다운로드-하기">첫번째 모델 다운로드 하기</h4>
<p>엔진을 설치했으니, 이제 작업에 적합한 LLM를 선택해야 합니다. 수천/수만개의 LLM들이 있는데, 다행히 Ollama는 다양한 목적과 크기를 가진 <strong>공식 라이브러리</strong>를 제공하고 있습니다. 여기에서 원하는 모델을 다운로드 해서 사용하면 됩니다.</p>

<p>🔗 <a href="https://ollama.com/library">Ollama 공식 Library</a> 페이지에서 LLM을 검색 할 수 있습니다.</p>

<p>우선은 작은 모델을 먼저 테스트해보겠습니다. 이번에는 작지만 강력한 그리고 무엇보다 한글도 잘 인식하는 구글의 Gemma3 4B 모델을 설치해보겠습니다.</p>

<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vShLtQ-zL_cEDUwfvHn-tuDOEol7M4tlaW_QUP2MPOUFdgcMml90bNGX_C7TYF71XcGDp4l6S0N7GB7/pub?w=960&amp;h=720" alt="Gemma3" /></p>

<p>Gemma3 4B 모델은 약 40억개의 파라미터를 가집니다. Ollama는 보통 4비트 양자화(q4)된 모델을 사용하는데, 이 경우 모델 자체를 불러오는데 대략 4GB 크기의 VRAM 을 가진 GPU가 필요합니다. 참고로 제 PC의 사양은 아래와 같습니다.</p>
<ul>
  <li>CPU: AMD Ryzen 5 5600X 6-Core</li>
  <li>Memory: 32G</li>
  <li>GPU: NVIDIA GeForce RTX 4060 Ti 16G 
가장 중요한 것은 GPU 가 되겟습니다. 16G 정도면 4비트 양자화된 20B 크기의 모델도 실행 할 수 있기 때문에 적당한 가성비 모델로 사용 할 수 있습니다.</li>
</ul>

<p>ollama 주요 명령들을 정리했습니다.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 1단계: 모델 다운로드
# gemma3:4b 모델을 다운로드 합니다. 시스템 사양이 좋지 않다면 gemma3:1b를 다운로드 해도 됩니다.
ollama pull gemma3:4b

# 2단계: 모델 실행
# 대화형 채팅이 열립니다.

# 3단계: 다운로드 한 모델목록을 나열합니다. 
ollama list

# 4단계: 실행 중인 모델을 확인합니다.
ollama ps

# 5단계: 모델을 중지 합니다.
ollama stop gemma3:4b

# 모델 삭제
ollama rm gemma3:4b
</code></pre></div></div>

<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vQkevYSdcq_pnRkOSppsifPpgJzH7y_19r1m_29zP5OLsZ6lTkxJz-dkGqY5uibutvYLOpW68FwZvFS/pub?w=960&amp;h=720" alt="Gemma3 다운로드" /></p>

<h2 id="ollama-api-사용하기">Ollama API 사용하기</h2>
<p>Ollama는 API를 제공하며 이를 이용해서, 다양한 앱 개발이 가능합니다.</p>
<ul>
  <li>내장 API 서버: Ollama는 별도의 설정 없이 http://localhost:11434 로 기본적인 백앤드 API 서버를 실행합니다.</li>
  <li>API를 이용해서 즉시 모델을 교체해서 사용 할 수 있습니다.</li>
</ul>

<p>예를 들어서 <code class="language-plaintext highlighter-rouge">GET /api/tags</code> API를 이용하면 모델 목록을 가져올 수 있습니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">curl</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">11434</span><span class="o">/</span><span class="n">api</span><span class="o">/</span><span class="n">tags</span> <span class="o">|</span> <span class="n">json_pp</span>
<span class="p">{</span>
   <span class="sh">"</span><span class="s">models</span><span class="sh">"</span> <span class="p">:</span> <span class="p">[</span>
      <span class="p">{</span>
         <span class="sh">"</span><span class="s">details</span><span class="sh">"</span> <span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">families</span><span class="sh">"</span> <span class="p">:</span> <span class="p">[</span>
               <span class="sh">"</span><span class="s">gemma3</span><span class="sh">"</span>
            <span class="p">],</span>
            <span class="sh">"</span><span class="s">family</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">"</span><span class="s">gemma3</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">format</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">"</span><span class="s">gguf</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">parameter_size</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">"</span><span class="s">4.3B</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">parent_model</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">""</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">quantization_level</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">"</span><span class="s">Q4_K_M</span><span class="sh">"</span>
         <span class="p">},</span>
         <span class="sh">"</span><span class="s">digest</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">"</span><span class="s">a2af6cc3eb7fa8be8504abaf9b04e88f17a119ec3f04a3addf55f92841195f5a</span><span class="sh">"</span><span class="p">,</span>
         <span class="sh">"</span><span class="s">model</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">"</span><span class="s">gemma3:4b</span><span class="sh">"</span><span class="p">,</span>
         <span class="sh">"</span><span class="s">modified_at</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">"</span><span class="s">2025-08-17T22:57:08.540920901+09:00</span><span class="sh">"</span><span class="p">,</span>
         <span class="sh">"</span><span class="s">name</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">"</span><span class="s">gemma3:4b</span><span class="sh">"</span><span class="p">,</span>
         <span class="sh">"</span><span class="s">size</span><span class="sh">"</span> <span class="p">:</span> <span class="mi">3338801804</span>
      <span class="p">},</span>
      <span class="p">{</span>
         <span class="sh">"</span><span class="s">details</span><span class="sh">"</span> <span class="p">:</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">families</span><span class="sh">"</span> <span class="p">:</span> <span class="p">[</span>
               <span class="sh">"</span><span class="s">gptoss</span><span class="sh">"</span>
            <span class="p">],</span>
            <span class="sh">"</span><span class="s">family</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">"</span><span class="s">gptoss</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">format</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">"</span><span class="s">gguf</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">parameter_size</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">"</span><span class="s">20.9B</span><span class="sh">"</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">parent_model</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">""</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">quantization_level</span><span class="sh">"</span> <span class="p">:</span> <span class="sh">"</span><span class="s">MXFP4</span><span class="sh">"</span>
         <span class="p">},</span>
<span class="p">...</span> <span class="n">생략</span>

</code></pre></div></div>

<p>기본적으로 Ollama의 API는 상태 비저장입니다. 즉, 모든 요청은 완전히 새로운 대화입니다. 사람처럼 대화하는 <strong>멀티턴</strong> 대화가 필요하다면, 코드로 이를 구현해야 합니다. 간단한 Python 코드 만들어보겠습니다.</p>

<h2 id="ollama-api를-이용한-llm-애플리케이션">Ollama API를 이용한 LLM 애플리케이션</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">requests</span>

<span class="c1"># Ollama's default API endpoint
</span><span class="n">OLLAMA_URL</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:11434/api/generate</span><span class="sh">"</span>

<span class="k">def</span> <span class="nf">chat_with_ai</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gemma3:4b</span><span class="sh">"</span><span class="p">):</span>
    <span class="n">payload</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">:</span> <span class="n">model</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">prompt</span><span class="sh">"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">stream</span><span class="sh">"</span><span class="p">:</span> <span class="bp">False</span>  <span class="c1"># Disable streaming for simplicity
</span>    <span class="p">}</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span><span class="n">OLLAMA_URL</span><span class="p">,</span> <span class="n">json</span><span class="o">=</span><span class="n">payload</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">response</span><span class="p">.</span><span class="nf">json</span><span class="p">()[</span><span class="sh">"</span><span class="s">response</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># Example usage
</span><span class="n">ai_response</span> <span class="o">=</span> <span class="nf">chat_with_ai</span><span class="p">(</span><span class="sh">"</span><span class="s">서울에서 가장 유명한 관광지 3군데 알려줘.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div>
<p>🤖 AI says: 서울에서 가장 유명한 관광지 3곳은 다음과 같습니다.</p>
<ol>
  <li><strong>경복궁 (Gyeongbokgung Palace):</strong> 조선 시대의 대표적인 궁궐로, 아름다운 건축물과 넓은 정원을 자랑합니다. 한복을 입고 궁궐을 거닐며 한국의 역사와 문화를 느껴볼 수 있습니다. 특히, 계절마다 다른 아름다움을 느낄 수 있는 꽃청춘 행사나 다양한 축제가 열리는 곳입니다.</li>
  <li><strong>명동 (Myeongdong):</strong> 쇼핑과 먹거리가 가득한 활기 넘치는 거리입니다. 다양한 브랜드 매장, 길거리 음식, 화장품 가게 등 볼거리와 즐길 거리가 다양합니다. 특히, 밤에는 화려한 조명으로 쇼핑몰이 더욱 아름답게 빛납니다.</li>
  <li><strong>남산 (Namsan Mountain):</strong> 서울 시내를 한눈에 내려다볼 수 있는 곳입니다. 케이블카를 타고 올라갈 수도 있으며, 남산타워에는 전망대와 레스토랑이 있습니다.  산책로를 따라 다양한 식물과 조형물을 감상하며 휴식을 취할 수 있습니다.
이 외에도 이화마을, 인사동, 종묘 등 서울에는 다양한 명소들이 많으니, 취향에 맞는 곳을 선택하여 즐거운 여행을 즐기세요!</li>
</ol>

<p><strong>Ollama Package</strong>를 이용해서 코드를 만들 수도 있습니다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># -*- coding: utf-8 -*-
</span><span class="kn">from</span> <span class="n">ollama</span> <span class="kn">import</span> <span class="n">chat</span>

<span class="c1"># First call - No memory
</span><span class="n">response</span> <span class="o">=</span> <span class="nf">chat</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">'</span><span class="s">gemma3:4b</span><span class="sh">'</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
    <span class="p">{</span><span class="sh">'</span><span class="s">role</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">user</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">왜 하늘은 파란가요?</span><span class="sh">'</span><span class="p">}</span>
<span class="p">])</span>
<span class="nf">print</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">message</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>
</code></pre></div></div>

<p>물론 <strong>LangChain</strong> 도 지원한다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># -*- coding: utf-8 -*-
</span><span class="kn">from</span> <span class="n">langchain_community.llms</span> <span class="kn">import</span> <span class="n">Ollama</span>

<span class="c1"># Initialize Ollama with your chosen model
</span><span class="n">llm</span> <span class="o">=</span> <span class="nc">Ollama</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gemma3:4b</span><span class="sh">"</span><span class="p">)</span>

<span class="n">prompt_1</span> <span class="o">=</span> <span class="sh">'</span><span class="s">상대성 이론에 대해서 설명해주세요</span><span class="sh">'</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">llm</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">prompt_1</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="openwebui">OpenWebUI</h2>
<p>OpenWebUI는 LLM 사용과 테스트를 도와주는 웹 애플리케이션입니다. OpenWebUI를 이용하면 OpenAI, Gemini 뿐만 아니라 Ollama 까지 손쉽게 사용 할 수 있습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run -d --network="host" -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
</code></pre></div></div>
<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vQkzS0hsBwzL2jRraQ16Q_M4eFBGg1HZyXrv0FFjB3c3_BE_hK5pxPKFcqnxyNt9v9v-79e33MB_i6S/pub?w=960&amp;h=720" alt="OpenWebUI" /></p>

<p>“사용자 아이콘 클릭 &gt; 관리자 설정 &gt; 연결” 에서 Ollama API Endpoint를 설정 할 수 있다. ⚙️ 버튼을 클릭해서 Ollama API 주소를 “http://localhost:11434”로 바꾸고 “연결 테스트”까지 진행해보자.
<img src="https://docs.google.com/drawings/d/e/2PACX-1vSSSOemDBsPe6ircPcNnWbyHsG8oxwpNnlw7w8eC6NjiJKPZ5FmrQf9d5kOgTXM3Vh6jYNrb6YWVs99/pub?w=960&amp;h=720" alt="OpenWebUI connection" /></p>

<p>아래와 같이 ollama에 설치된 모델들을 선택해서 AI와 대화 할 수 있다.
<img src="https://docs.google.com/drawings/d/e/2PACX-1vRNqLt-v1DVwyZ_HL1qtbzDgKPPfnlhN5BdprEb2bVAon2QxmW_l5Yl-vOfp4k8JPKb6_sdMLla-zeL/pub?w=960&amp;h=720" alt="OpenWebUI Ollama 모델 선택" /></p>

<h2 id="추천-모델">추천 모델</h2>
<h4 id="gemma-3">Gemma 3</h4>
<p>구글의 gemma 3 모델은 gemini 기술을 기반으로 개발된 오픈소스 모델입니다. 1B, 4B, 12B, 27B 등의 크기를 제공하는데, 로컬 PC에서도 작동할 수 있도록 최적화되어 있습니다. 4B 이상 모델은 이미지와 비디오를 처리할 수 있는 <strong>멀티모달</strong> 기능을 제공하고 있습니다. 특히 “한글” 처리 능력이 매우 뛰어납니다.</p>

<p>이미지 분석을 요청해봤습니다.</p>

<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vSDIpXQNIPlLN8lhHTKJBBF8NZJ8B6JhPnji38B430tOkRpAZx0u5A-_RPj9cb3HGm0cGT0IgskVz2P/pub?w=960&amp;h=720" alt="패션 이미지" /></p>

<blockquote>
  <p>모델이 입고 있는 옷에 대해서 분석해주세요.</p>
</blockquote>

<p>🤖 모델이 입고 있는 옷에 대한 분석입니다.</p>

<p><strong>전체적인 스타일:</strong>
모델의 스타일은 1950년대 레트로 패션과 현대적인 감각이 조화된 느낌입니다. 특히, 핀업 스타일의 요소를 차용하여 여성스러움과 귀여움을 동시에 표현하고 있습니다.</p>

<p><strong>세부적인 분석:</strong></p>
<ul>
  <li><strong>셔츠:</strong> 빨간색과 흰색의 격자무늬 셔츠를 입고 있으며, 셔츠의 단추를 느슨하게 열어 여성스러운 분위기를 강조했습니다. 셔츠의 칼라와 소매 부분은 핀업 스타일의 특징을 보여줍니다.</li>
  <li><strong>하의:</strong> 밝은 색상의 데님 하의를 입고 있으며, 허리 부분은 묶어서 스타일을 연출했습니다.</li>
  <li><strong>액세서리:</strong> 머리 부분에는 빨간색 리본을 착용하여 포인트를 주었습니다. 립스틱은 강렬한 빨간색으로 포인트가 되어 있습니다.</li>
</ul>

<p><strong>전체적인 인상:</strong>
모델의 의상은 복고풍의 매력과 현대적인 세련됨이 어우러진 스타일입니다. 밝고 경쾌한 분위기를 자아내며, 모델의 개성을 잘 드러내고 있습니다.</p>

<blockquote>
  <p>모델의 옷 스타일이 1950년대 핀업 스타일에서 영감을 받았다고 하셨는데, 구체적으로 어떤 요소들이 그렇게 보이게 하는지 더 자세히 설명해주실 수 있나요?</p>
</blockquote>

<p>🤖 네, 모델의 옷 스타일이 1950년대 핀업 스타일에서 영감을 받았다고 말씀드린 이유를 더 자세히 설명드리겠습니다. 핀업 스타일은 1940년대 후반부터 1950년대 초반에 큰 인기를 끌었던 스타일로, 여성의 매력을 과감하고 관능적으로 표현하는 것이 특징입니다. 모델의 스타일에서 보이는 핀업 스타일의 요소는 다음과 같습니다.</p>

<ol>
  <li>
    <p>셔츠의 스타일:
높은 칼라와 둥근 소매: 1950년대 핀업 스타일에서 자주 볼 수 있는 요소입니다. 셔츠의 칼라는 목을 감싸는 형태로, 여성의 목선을 강조합니다. 둥근 소매는 부드러운 느낌을 더해줍니다.
단추를 느슨하게 연 것: 셔츠의 단추를 느슨하게 연 것은 핀업 스타일의 대표적인 특징입니다. 이는 여성의 몸매를 살짝 드러내어 관능적인 분위기를 연출합니다.</p>
  </li>
  <li>
    <p>전체적인 실루엣:
허리 라인 강조: 핀업 스타일은 여성의 곡선을 돋보이게 하는 것이 중요합니다. 모델의 스타일은 허리 라인을 살짝 강조하여 여성의 아름다움을 부각합니다.
전체적으로 귀여운 느낌: 핀업 스타일은 단순히 관능적인 것뿐만 아니라, 귀여운 매력도 함께 어필합니다. 모델의 스타일은 둥근 소매와 밝은 색상의 하의를 통해 이러한 귀여운 느낌을 살립니다.</p>
  </li>
  <li>
    <p>액세서리:
빨간색 리본: 머리에 착용한 빨간색 리본은 핀업 스타일에서 자주 사용되는 액세서리입니다. 이는 여성의 귀여움을 강조하고, 전체적인 스타일에 포인트를 더합니다.
강렬한 립스틱: 붉은색 립스틱은 핀업 스타일의 상징과 같습니다. 이는 여성의 입술을 강조하여 관능적인 분위기를 연출합니다.
이러한 요소들이 결합되어 모델의 스타일은 1950년대 핀업 스타일에서 영감을 받은 스타일임을 보여줍니다.</p>
  </li>
</ol>

<hr />
<p>이-커머스에서 상품 정보를 분석하는 목적으로 사용하면 좋을 것 같습니다.</p>

<h4 id="qwen-3">Qwen 3</h4>
<p>자연스럽게 한글이 잘 되는 모델을 찾게 되더군요. Qwen 시리즈는 한글을 가장 잘 처리하는 모델 중 하나 입니다. 한글 분석 능력을 테스트 해봤습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>고객 제품 후기 문장에서 “제품”, “감성”, “평가” 정보를 추출해주세요.

고객 제품 후기
새로 산 스마트폰, 정말 대만족입니다! 디자인이 매트해서 지문이 잘 안 묻는 점이 특히 좋고, 카메라 기능이 확실히 업그레이드되어서 밤에도 선명하게 사진이 찍히네요. 배터리가 오래가는 건 물론이고, 충전 속도도 빨라서 바쁜 아침에 잠깐 충전해도 하루 종일 쓸 수 있어요. 이전 모델보다 훨씬 가벼워서 손목에 무리가 가지 않는 것도 장점이고요. 전반적으로 모든 면에서 아주 만족스러운 구매였습니다
</code></pre></div></div>

<p>🤖 추출 정보</p>
<ul>
  <li>제품: 스마트폰</li>
  <li>감성: 긍정적 (대만족, 좋다, 만족스러운 구매 등)</li>
  <li>평가:
    <ul>
      <li>디자인: 매트 처리로 지문이 잘 묻지 않음</li>
      <li>카메라 기능: 밤에도 선명한 사진 촬영 가능 (업그레이드됨)</li>
      <li>배터리: 오래가고, 충전 속도 빠름 (바쁜 아침에 잠깐 충전으로 하루 사용 가능)</li>
      <li>무게: 이전 모델보다 가벼워서 손목에 무리가 적음</li>
      <li>전반적 만족도: 모든 면에서 만족스러운 구매</li>
    </ul>
  </li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>이 무선 이어폰은 디자인은 예쁜데, 사용성이 너무 떨어져요. 착용감이 좋지 않아서 조금만 오래 끼고 있으면 귀가 아프고, 블루투스 연결이 자꾸 끊겨서 중요한 통화 중에 답답한 상황이 여러 번 발생했습니다. 음질은 평범한 수준이고, 특히 통화 시 상대방에게 제 목소리가 잘 안 들린다고 하더라고요. 충전 케이스도 충전이 제대로 안 되는 문제가 있어서 결국 AS를 맡겨야 할 것 같습니다. 정말 후회되는 구매네요
</code></pre></div></div>

<p>🤖 추출 정보</p>
<ul>
  <li>제품: 무선 이어폰</li>
  <li>감성: 부정적 (후회, 답답, 아프다, 불만 등)</li>
  <li>평가:
    <ul>
      <li>디자인: 예쁘다 (긍정적 평가)</li>
      <li>사용성:
        <ul>
          <li>착용감: 오래 착용 시 귀가 아프다</li>
          <li>블루투스 연결: 자주 끊김 (중요한 통화 중 문제 발생)</li>
        </ul>
      </li>
      <li>음질: 평범한 수준 (특히 통화 시 목소리 전달 문제 발생)</li>
      <li>충전 케이스: 충전 불안정 (AS 요청 필요)</li>
      <li>전체적 평가: 후회되는 구매 (다양한 문제로 인한 불만)</li>
    </ul>
  </li>
</ul>

<h2 id="다음에-할-일들">다음에 할 일들</h2>
<ul>
  <li>llama3, Qwen 과 같은 다른 모델들을 테스트해보세요</li>
  <li>LangChain/LanGraph의 retrieval, agents, memory 기능을 이용해서 LLM 애플리케이션을 만들어보세요.</li>
  <li>Gemma3 270M 같은 작은 모델을 이용해서 파인튜닝 해보세요.</li>
  <li>팀을 위한 로컬 기반의 챗봇을 배포해보세요.</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#llm" class="page__taxonomy-item p-category" rel="tag">LLM</a><span class="sep">, </span>
    
      <a href="/categories/#ollama" class="page__taxonomy-item p-category" rel="tag">ollama</a><span class="sep">, </span>
    
      <a href="/categories/#sllm" class="page__taxonomy-item p-category" rel="tag">sLLM</a><span class="sep">, </span>
    
      <a href="/categories/#%EC%83%9D%EC%84%B1%ED%98%95-ai" class="page__taxonomy-item p-category" rel="tag">생성형-AI</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2025-08-19T01:00:00+09:00">August 19, 2025</time></p>

      </footer>

      <section class="page__share">
  <h4 class="page__share-title">Share on</h4>

  <a href="https://x.com/intent/tweet?text=Local+LLM+%ED%98%B8%EC%8A%A4%ED%8C%85%EC%9D%84+%EC%9C%84%ED%95%9C+ollma+%EA%B0%80%EC%9D%B4%EB%93%9C%20%2Fllm%2Follama%2Fsllm%2F%25EC%2583%259D%25EC%2584%25B1%25ED%2598%2595-ai%2Follala-101%2F" class="btn btn--x" aria-label="Share on X" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on X">
    <i class="fab fa-fw fa-x-twitter" aria-hidden="true"></i><span> X</span>
  </a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=%2Fllm%2Follama%2Fsllm%2F%25EC%2583%259D%25EC%2584%25B1%25ED%2598%2595-ai%2Follala-101%2F" class="btn btn--facebook" aria-label="Share on Facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook">
    <i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span>
  </a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=/llm/ollama/sllm/%EC%83%9D%EC%84%B1%ED%98%95-ai/ollala-101/" class="btn btn--linkedin" aria-label="Share on LinkedIn" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn">
    <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span>
  </a>

  <a href="https://bsky.app/intent/compose?text=Local+LLM+%ED%98%B8%EC%8A%A4%ED%8C%85%EC%9D%84+%EC%9C%84%ED%95%9C+ollma+%EA%B0%80%EC%9D%B4%EB%93%9C%20%2Fllm%2Follama%2Fsllm%2F%25EC%2583%259D%25EC%2584%25B1%25ED%2598%2595-ai%2Follala-101%2F" class="btn btn--bluesky" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Bluesky">
    <i class="fab fa-fw fa-bluesky" aria-hidden="true"></i><span> Bluesky</span>
  </a>
</section>


      
  <nav class="pagination">
    
      <a href="/llm/%EC%83%9D%EC%84%B1%ED%98%95-ai/quick/lab/gemini-use/" class="pagination--pager" title="Gemini 사용에 대한 코드 정리(VertexAI vs GenAI)">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/llm/%EC%83%9D%EC%84%B1%ED%98%95-ai/quick/lab/gemini-use/" rel="permalink">Gemini 사용에 대한 코드 정리(VertexAI vs GenAI)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">라이브러리 Import

  공통적으로 사용하는 라이브러리를 불러옵니다.

</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/llm/%EC%83%9D%EC%84%B1%ED%98%95-ai/ai-agent/vertical-ai/ai-%ED%8A%B8%EB%A0%8C%EB%93%9C/ai-trends/" rel="permalink">8월 3째주 AI 뉴스 및 트렌드
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">GPT-5 정식 출시


  
  
    
      
    
  
  
  
    
      OpenAI ChatGPT-5 launch live updates — all the latest news and biggest upgrades
      Everything you...</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/llm/%EC%83%9D%EC%84%B1%ED%98%95-ai/ai-%EC%BD%94%EB%94%A9/vibe-coding/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81/vibe-coding/" rel="permalink">Vibe 코딩 한달살이
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          10 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">AI 코딩 어시스턴트
AI 어이스턴트를 이용한 개발은 대략 2021년 이후 일상화된 것으로 알려져 있다. OpenAI Codex, GitHub Copilot 코딩 AI 어시스턴트의 시작을 알렸고, 그 이후 Cursor, Windsurf, Replit AI, Claude Code, ...</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/llm/%EC%83%9D%EC%84%B1%ED%98%95-ai/gpt-oss/ollama/ai-%ED%8A%B8%EB%A0%8C%EB%93%9C/chat-gpt-oss/" rel="permalink">GPT-OSS 사용기
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          19 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">새로운 오픈소스 LLM의 등장
OpenAI가 2019년 GPT-2를 시장에 내놓은 이후 6년만에 드디어 오픈소스 모델인 GPT-OSS(gpt-oss-20b, gpt-oss-120b)를 공개했습니다. OpenAI가 그동안 내놓았던 GPT-3, GPT-4는 모두 독점모델(Closed-...</p>
  </article>
</div>

    
  </div>
</div>

  
  

<script src="https://giscus.app/client.js"
        data-repo="joinc-dream/joinc-dream.github.io"
        data-repo-id="R_kgDOPa3ASQ"
        data-category="General"
        data-category-id="DIC_kwDOPa3ASc4CuAGK"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="light"
        data-lang="ko"
        crossorigin="anonymous"
        async>
</script>
</div>


    
  </div>

  

  <div id="footer" class="page__footer">
    <footer>
      <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
      

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2013 - 2025 <a href="">JOINC AI TeamBlog</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

    </footer>
  </div>

  
  <script src="/assets/js/main.min.js"></script>









</body>

</html>
